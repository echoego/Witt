{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive coding framework \n",
    "##Including sequence modeling, action environment interaction and mutiagent language learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.functional import F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as dst\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "\n",
    "img_x=240\n",
    "img_y=320\n",
    "\n",
    "def conv2D_output_size(img_size, padding, kernel_size, stride):\n",
    "    # compute output shape of conv2D\n",
    "    outshape = (np.floor((img_size[0] + 2 * padding[0] - (kernel_size[0] - 1) - 1) / stride[0] + 1).astype(int),\n",
    "                np.floor((img_size[1] + 2 * padding[1] - (kernel_size[1] - 1) - 1) / stride[1] + 1).astype(int))\n",
    "    return outshape\n",
    "\n",
    "#二维卷积\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, img_x=img_x, img_y=img_y, fc_hidden1=512, fc_hidden2=512, drop_p=0.3, CNN_embed_dim=300):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "\n",
    "        self.img_x = img_x\n",
    "        self.img_y = img_y\n",
    "        self.CNN_embed_dim = CNN_embed_dim\n",
    "\n",
    "        # CNN architechtures\n",
    "        self.ch1, self.ch2, self.ch3, self.ch4 = 32, 64, 128, 256\n",
    "        self.k1, self.k2, self.k3, self.k4 = (5, 5), (3, 3), (3, 3), (3, 3)      # 2d kernal size\n",
    "        self.s1, self.s2, self.s3, self.s4 = (2, 2), (2, 2), (2, 2), (2, 2)      # 2d strides\n",
    "        self.pd1, self.pd2, self.pd3, self.pd4 = (0, 0), (0, 0), (0, 0), (0, 0)  # 2d padding\n",
    "\n",
    "        # conv2D output shapes\n",
    "        self.conv1_outshape = conv2D_output_size((self.img_x, self.img_y), self.pd1, self.k1, self.s1)  # Conv1 output shape\n",
    "        self.conv2_outshape = conv2D_output_size(self.conv1_outshape, self.pd2, self.k2, self.s2)\n",
    "        self.conv3_outshape = conv2D_output_size(self.conv2_outshape, self.pd3, self.k3, self.s3)\n",
    "        self.conv4_outshape = conv2D_output_size(self.conv3_outshape, self.pd4, self.k4, self.s4)\n",
    "\n",
    "        # fully connected layer hidden nodes\n",
    "        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\n",
    "        self.drop_p = drop_p\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=self.ch1, kernel_size=self.k1, stride=self.s1, padding=self.pd1),\n",
    "            nn.BatchNorm2d(self.ch1, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),                      \n",
    "            # nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.ch1, out_channels=self.ch2, kernel_size=self.k2, stride=self.s2, padding=self.pd2),\n",
    "            nn.BatchNorm2d(self.ch2, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.ch2, out_channels=self.ch3, kernel_size=self.k3, stride=self.s3, padding=self.pd3),\n",
    "            nn.BatchNorm2d(self.ch3, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.ch3, out_channels=self.ch4, kernel_size=self.k4, stride=self.s4, padding=self.pd4),\n",
    "            nn.BatchNorm2d(self.ch4, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.drop = nn.Dropout2d(self.drop_p)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(self.ch4 * self.conv4_outshape[0] * self.conv4_outshape[1], self.fc_hidden1)   # fully connected layer, output k classes\n",
    "        self.fc2 = nn.Linear(self.fc_hidden1, self.fc_hidden2)\n",
    "        self.fc3 = nn.Linear(self.fc_hidden2, self.CNN_embed_dim)   # output = CNN embedding latent variables\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "            # CNNs\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.view(x.size(0), -1)           # flatten the output of conv\n",
    "\n",
    "        # FC layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "        out = self.fc3(x)\n",
    "            \n",
    "        # swap time and sample dim such that (sample dim, time dim, CNN latent dim)\n",
    "        # cnn_embed_seq: shape=(batch, time_step, input_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "#class: sequence coding module\n",
    "#惰性编码单元\n",
    "class inertiaCodingCell(nn.Module):\n",
    "    '''\n",
    "    input:(batch,seq_len,channel,witdth,height)\n",
    "    output:(batch,code_len,code_dim)\n",
    "    \n",
    "    '''\n",
    "    def __init__(self,n_channel=1,width=img_x,height=img_y):\n",
    "        super(inertiaCodingCell,self).__init__()\n",
    "        self.encoder_l=EncoderCNN(CNN_embed_dim=1024)\n",
    "        self.d_f1=nn.Linear(in_features=1024,out_features=1024)\n",
    "        self.d_fE=nn.Linear(in_features=1024,out_features=256)\n",
    "        self.d_fS=nn.Linear(in_features=1024,out_features=256)\n",
    "        \n",
    "        self.pred_f1=nn.Linear(in_features=512,out_features=512)\n",
    "        self.pred_f2=nn.Linear(in_features=512,out_features=256)\n",
    "        \n",
    "        self.decoder_fl=nn.Linear(in_features=512,out_features=56*75*3)\n",
    "        self.decoder_f2=nn.ConvTranspose2d(in_channels=3,out_channels=64,kernel_size=(7,7),stride=2)\n",
    "        self.decoder_f3=nn.ConvTranspose2d(in_channels=64,out_channels=3,kernel_size=(8,12),stride=2)\n",
    "\n",
    "                \n",
    "    def encoder(self,x):\n",
    "        h=self.encoder_l(x)\n",
    "        h=self.d_f1(h)\n",
    "        h=F.relu(h)\n",
    "        E=self.d_fE(h)\n",
    "        E=F.relu(E)\n",
    "        S=self.d_fS(h)\n",
    "        S=F.relu(S)\n",
    "        return E,S\n",
    "    \n",
    "    def predictor(self,E,S):\n",
    "        temp=torch.cat([E,S],1)\n",
    "        temp=self.pred_f1(temp)\n",
    "        temp=F.relu(temp)\n",
    "        out=self.pred_f2(temp)\n",
    "        out=F.relu(out)\n",
    "        return(out)\n",
    "\n",
    "    def decoder(self,E,S):\n",
    "        temp=torch.cat([E,S],1)\n",
    "        temp=self.decoder_fl(temp)\n",
    "        temp=temp.view(-1,3,56,75)\n",
    "        temp=self.decoder_f2(temp)\n",
    "        x_rec=self.decoder_f3(temp)\n",
    "        return(x_rec)\n",
    "        \n",
    "    def forward(self,x,pre_E=None,pred_S=None):\n",
    "        \n",
    "        if (pre_E is None) or (pred_S is None):\n",
    "            \n",
    "            x_in=x\n",
    "            E_plus,S_plus=self.encoder(x_in)\n",
    "            E=E_plus\n",
    "            S=S_plus\n",
    "            x_rec=self.decoder(E,S)\n",
    "            S_pred=self.predictor(E,S)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            x_pred_rec=self.decoder(pre_E,pred_S)\n",
    "            x_in=x-x_pred_rec\n",
    "            E_plus,S_plus=self.encoder(x_in)\n",
    "            E=pre_E+E_plus\n",
    "            S=pred_S+S_plus\n",
    "            x_rec=self.decoder(E,S)\n",
    "            S_pred=self.predictor(E,S)\n",
    "        \n",
    "        return(E,S,S_pred,E_plus,x_rec)\n",
    "    \n",
    "#惰性编码预测网络\n",
    "class inertiaCodingNet(nn.Module):\n",
    "    def __init__(self,seq_len=10,n_channel=3,w=img_x,h=img_y):\n",
    "        super(inertiaCodingNet,self).__init__()\n",
    "        self.seq_len=seq_len\n",
    "        self.n_channel=n_channel\n",
    "        self.w=w\n",
    "        self.h=h\n",
    "        self.inertiaCell=inertiaCodingCell()\n",
    "        \n",
    "        \n",
    "    \n",
    "    #input (batch_size,seq_len,n_channel,w,h)\n",
    "    def forward(self,x):\n",
    "        b,s_len,c_len,w,h=x.shape\n",
    "        E=[]\n",
    "        S=[]\n",
    "        S_pred=[]\n",
    "        E_plus=[]\n",
    "        x_rec=[]\n",
    "        for i in range(s_len):\n",
    "            x_t=x[:,i,:,:,:]\n",
    "            \n",
    "            if i==0:\n",
    "                E_t,S_t,S_pred_t,E_plus_t,x_rec_t=self.inertiaCell(x_t)\n",
    "            else:\n",
    "                E_t,S_t,S_pred_t,E_plus_t,x_rec_t=self.inertiaCell(x_t,pre_E=E_t,pred_S=S_pred_t)\n",
    "            \n",
    "            E.append(E_t)\n",
    "            S.append(S_t)\n",
    "            S_pred.append(S_pred_t)\n",
    "            E_plus.append(E_plus_t)\n",
    "            x_rec.append(x_rec_t)\n",
    "            \n",
    "        E=torch.stack(E).transpose(0,1)\n",
    "        S=torch.stack(S).transpose(0,1)\n",
    "        E_plus=torch.stack(E_plus).transpose(0,1)\n",
    "        x_rec=torch.stack(x_rec).transpose(0,1)\n",
    "            \n",
    "        #(batch_size,seq_len,hidden_dim)\n",
    "        return(E,S,E_plus,x_rec)\n",
    "        \n",
    "        \n",
    "#方差损失\n",
    "\n",
    "def var_loss(tensor):\n",
    "    temp=tensor\n",
    "    means=torch.mean(temp,axis=1).view(tensor.shape[0],1,-1)\n",
    "    return torch.sum(torch.pow(temp-means,2)) \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "## ------------------- label conversion tools ------------------ ##\n",
    "def labels2cat(label_encoder, list):\n",
    "    return label_encoder.transform(list)\n",
    "\n",
    "def labels2onehot(OneHotEncoder, label_encoder, list):\n",
    "    return OneHotEncoder.transform(label_encoder.transform(list).reshape(-1, 1)).toarray()\n",
    "\n",
    "def onehot2labels(label_encoder, y_onehot):\n",
    "    return label_encoder.inverse_transform(np.where(y_onehot == 1)[1]).tolist()\n",
    "\n",
    "def cat2labels(label_encoder, y_cat):\n",
    "    return label_encoder.inverse_transform(y_cat).tolist()\n",
    "\n",
    "\n",
    "class Dataset_CRNN(data.Dataset):\n",
    "    \"Characterizes a dataset for PyTorch\"\n",
    "    def __init__(self, data_path, frames,labels=None,transform=None,load_all=False):\n",
    "        \"Initialization\"\n",
    "        self.data_path = data_path\n",
    "        #self.labels = labels\n",
    "        folders=[]\n",
    "        for f in os.listdir(data_path):\n",
    "            if f!='.DS_Store':\n",
    "                folders.extend(list(map(lambda x:f+'/'+x,os.listdir(data_path+f))))\n",
    "                \n",
    "        for i in range(len(folders)-1,-1,-1):\n",
    "            if '.DS_Store' in folders[i]:\n",
    "                folders.pop(i)\n",
    "        \n",
    "        self.folders = folders\n",
    "        self.load_all=load_all\n",
    "        self.transform = transform\n",
    "        self.frames = frames\n",
    "        \n",
    "        if self.load_all:\n",
    "            temp=[]\n",
    "            print('loading all images')\n",
    "            for f in self.folders:\n",
    "                temp.append(self.read_images(self.data_path, f, self.transform) )\n",
    "            self.dataset=temp\n",
    "        \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return len(self.folders)\n",
    "\n",
    "    def read_images(self, path, selected_folder, use_transform):\n",
    "        \n",
    "        X = []\n",
    "        for i in self.frames:\n",
    "            image = Image.open(os.path.join(path, selected_folder, 'frame'+str(i)+'.jpg'))\n",
    "\n",
    "            if use_transform is not None:\n",
    "                image = use_transform(image)\n",
    "\n",
    "            X.append(image)\n",
    "        X = torch.stack(X, dim=0)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generates one sample of data\"\n",
    "        # Select sample\n",
    "        if not self.load_all:\n",
    "            folder = self.folders[index]\n",
    "\n",
    "        # Load data\n",
    "            X = self.read_images(self.data_path, folder, self.transform)     # (input) spatial images\n",
    "        #y = torch.LongTensor([self.labels[index]])                  # (labels) LongTensor are for int64 instead of FloatTensor\n",
    "        else:\n",
    "            X=self.dataset[index]\n",
    "        return X\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading all images\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "d_path='/Users/lekang/anaconda/tests/Review/Torch/predictiveCoding/ucf101-jpg/'\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize([240, 320]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "datatest=Dataset_CRNN(data_path=d_path,\n",
    "                      frames=list(range(1,21)),\n",
    "                      transform=transform,\n",
    "                      load_all=True\n",
    "                     )\n",
    "\n",
    "use_cuda=False\n",
    "batch_size=20\n",
    "\n",
    "all_data_params = {'batch_size': batch_size, 'shuffle': False, 'num_workers': 12}#, 'pin_memory': True}# if use_cuda else {}\n",
    "all_data_loader = data.DataLoader(datatest, \n",
    "                                  **all_data_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,datai in enumerate(all_data_loader):\n",
    "    print(datai.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10, 256])\n",
      "torch.Size([10, 10, 256])\n",
      "torch.Size([10, 10, 256])\n",
      "torch.Size([10, 10, 3, 240, 320])\n",
      "CPU times: user 39.5 s, sys: 1.29 s, total: 40.8 s\n",
      "Wall time: 7.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "code_net=inertiaCodingNet()#.to_device(device)\n",
    "\n",
    "a=torch.tensor(np.random.rand(10*240*320*3*10).reshape(10,10,3,240,320),dtype=torch.float32)#.to_device(device)\n",
    "E,H,E_plus,x_rec=code_net(a)\n",
    "print(E.shape)\n",
    "print(H.shape)\n",
    "print(E_plus.shape)\n",
    "print(x_rec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(434.7973, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import *\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# set path\n",
    "data_path = \"ucf101-jpg/\"    # define UCF-101 RGB data path\n",
    "action_name_path = \"./UCF101actions.pkl\"\n",
    "save_model_path = \"./checkpoints/\"\n",
    "\n",
    "# use same encoder CNN saved!\n",
    "CNN_fc_hidden1, CNN_fc_hidden2 = 1024, 768\n",
    "CNN_embed_dim = 512   # latent dim extracted by 2D CNN\n",
    "img_x, img_y = 256, 342  # resize video 2d frame size\n",
    "dropout_p = 0.0       # dropout probability\n",
    "\n",
    "# use same decoder RNN saved!\n",
    "RNN_hidden_layers = 3\n",
    "RNN_hidden_nodes = 512\n",
    "RNN_FC_dim = 256\n",
    "\n",
    "# training parameters\n",
    "k = 101             # number of target category\n",
    "batch_size = 40\n",
    "# Select which frame to begin & end in videos\n",
    "begin_frame, end_frame, skip_frame = 1, 29, 1\n",
    "\n",
    "\n",
    "with open(action_name_path, 'rb') as f:\n",
    "    action_names = pickle.load(f)   # load UCF101 actions names\n",
    "\n",
    "# convert labels -> category\n",
    "le = LabelEncoder()\n",
    "le.fit(action_names)\n",
    "\n",
    "# show how many classes there are\n",
    "list(le.classes_)\n",
    "\n",
    "# convert category -> 1-hot\n",
    "action_category = le.transform(action_names).reshape(-1, 1)\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(action_category)\n",
    "\n",
    "# # example\n",
    "# y = ['HorseRace', 'YoYo', 'WalkingWithDog']\n",
    "# y_onehot = labels2onehot(enc, le, y)\n",
    "# y2 = onehot2labels(le, y_onehot)\n",
    "\n",
    "actions = []\n",
    "fnames = os.listdir(data_path)\n",
    "\n",
    "all_names = []\n",
    "for f in fnames:\n",
    "    loc1 = f.find('v_')\n",
    "    loc2 = f.find('_g')\n",
    "    actions.append(f[(loc1 + 2): loc2])\n",
    "\n",
    "    all_names.append(f)\n",
    "\n",
    "\n",
    "# list all data files\n",
    "all_X_list = all_names              # all video file names\n",
    "all_y_list = labels2cat(le, actions)    # all video labels\n",
    "\n",
    "# data loading parameters\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize([img_x, img_y]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "selected_frames = np.arange(begin_frame, end_frame, skip_frame).tolist()\n",
    "\n",
    "# reset data loader\n",
    "all_data_params = {'batch_size': batch_size, 'shuffle': False, 'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "all_data_loader = data.DataLoader(Dataset_CRNN(data_path, all_X_list, all_y_list, selected_frames, transform=transform), **all_data_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from pathos.multiprocessing import ProcessingPool as P\n",
    "import os\n",
    "import scipy\n",
    "import scipy.misc\n",
    "\n",
    "\n",
    "#Image.open(os.path.join(path, selected_folder, 'frame{:06d}.jpg'.format(i))).convert('L')\n",
    "\n",
    "def convert_v2imag(inpath,outpath):\n",
    "    files=os.listdir(inpath)\n",
    "    for f in files:\n",
    "        \n",
    "        if not os.path.isdir(outpath):\n",
    "            os.mkdir(outpath)\n",
    "        \n",
    "        outfolder=outpath+'/'+f.split('.')[0]\n",
    "        os.chdir(outpath)\n",
    "        \n",
    "        os.mkdir(outfolder)\n",
    "        file=imageio.get_reader(inpath+'/'+f, \"ffmpeg\")\n",
    "        print(f)\n",
    "        for i in range(len(list(file))):\n",
    "            im_array=list(file)[i]\n",
    "            scipy.misc.imsave(outfolder+'/'+'frame'+str(i)+'.jpg', im_array)\n",
    "\n",
    "inpath_header='/Users/lekang/anaconda/tests/Review/Torch/predictiveCoding/UCF-101/'\n",
    "outpath_header='/Users/lekang/anaconda/tests/Review/Torch/predictiveCoding/ucf101-jpg/'\n",
    "\n",
    "def convert_folder(inpath_h,outpath_h,threads=6):\n",
    "    \n",
    "    p=P(threads)\n",
    "    \n",
    "    folders=os.listdir(inpath_h)\n",
    "    \n",
    "    p.map(lambda x:convert_v2imag(inpath_h+'/'+x,outpath_h+'/'+x),folders)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "convert_folder(inpath_h='/Users/lekang/anaconda/tests/Review/Torch/predictiveCoding/UCF-101/',\n",
    "               outpath_h='/Users/lekang/anaconda/tests/Review/Torch/predictiveCoding/ucf101-jpg/',threads=12)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "#x=np.stack(list(vd)[:100],0).transpose(0,3,1,2).reshape(10,10,3,240,-1)\n",
    "#x=torch.tensor(x,dtype=torch.float32)\n",
    "im1=Image.open('/Users/lekang/anaconda/tests/Review/Torch/predictiveCoding/ucf101-jpg/BlowDryHair/v_BlowDryHair_g01_c03/frame3.jpg')\n",
    "transform = transforms.Compose([transforms.Resize([240, 320]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "#transforms.ToTensor()ba\n",
    "transform(im1).shape\n",
    "save_image(transform(im1),'/Users/lekang/Desktop/tewsting.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lekang/anaconda/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import save_image\n",
    "import scipy.misc\n",
    "\n",
    "scipy.misc.imsave('/Users/lekang/Desktop/testingsave2.jpg', np.array(list(vd)[60]),)\n",
    "\n",
    "image_output = Image.fromarray(np.array(list(vd)[60].transpose(2,0,1)[0,:,:]))\n",
    "image_output.save('/Users/lekang/Desktop/testingsave1.jpg')\n",
    "\n",
    "save_image(torch.tensor(list(vd)[60].transpose(2,0,1),dtype=torch.float32),'/Users/lekang/Desktop/testingsave.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.6 s, sys: 673 ms, total: 40.3 s\n",
      "Wall time: 6.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "E,H,E_plus,x_rec=code_net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 240, 320])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/lekang/Downloads/UCF101 - Action Recognition Data Set/UCF-101/Archery/frame000000.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-48d485b0c7c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                )\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0md1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-113-48d485b0c7c2>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# (input) spatial images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# (labels) LongTensor are for int64 instead of FloatTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-48d485b0c7c2>\u001b[0m in \u001b[0;36mread_images\u001b[0;34m(self, path, selected_folder, use_transform)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'frame{:06d}.jpg'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2580\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2581\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/lekang/Downloads/UCF101 - Action Recognition Data Set/UCF-101/Archery/frame000000.jpg'"
     ]
    }
   ],
   "source": [
    "\n",
    "class Dataset_CRNN(data.Dataset):\n",
    "    \"Characterizes a dataset for PyTorch\"\n",
    "    def __init__(self, data_path, folders, labels, frames, transform=None):\n",
    "        \"Initialization\"\n",
    "        self.data_path = data_path\n",
    "        self.labels = labels\n",
    "        self.folders = folders\n",
    "        self.transform = transform\n",
    "        self.frames = frames\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return len(self.folders)\n",
    "\n",
    "    def read_images(self, path, selected_folder, use_transform):\n",
    "        X = []\n",
    "        for i in self.frames:\n",
    "            image = Image.open(os.path.join(path, selected_folder, 'frame{:06d}.jpg'.format(i)))\n",
    "\n",
    "            if use_transform is not None:\n",
    "                image = use_transform(image)\n",
    "\n",
    "            X.append(image)\n",
    "        X = torch.stack(X, dim=0)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generates one sample of data\"\n",
    "        # Select sample\n",
    "        folder = self.folders[index]\n",
    "\n",
    "        # Load data\n",
    "        X = self.read_images(self.data_path, folder, self.transform)     # (input) spatial images\n",
    "        y = torch.LongTensor([self.labels[index]])                  # (labels) LongTensor are for int64 instead of FloatTensor\n",
    "\n",
    "        # print(X.shape)\n",
    "        return X, y\n",
    "    \n",
    "d1=Dataset_CRNN(data_path='/Users/lekang/Downloads/UCF101 - Action Recognition Data Set/UCF-101/',\n",
    "                folders=['Archery'],\n",
    "                labels=['Archery'],\n",
    "                frames=range(29),\n",
    "                \n",
    "               )\n",
    "d1[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frame000003.jpg'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
